Situation Assessment:

%We present ROBOSHERLOCK, an open source
software framework for implementing perception systems for
robots performing human-scale everyday manipulation tasks.
In ROBOSHERLOCK, perception and interpretation of realistic
scenes is formulated as an unstructured information manage-
ent (UIM) problem. The application of the UIM principle
supports the implementation of perception systems that can
answer task-relevant queries about objects in a scene, boost
object recognition performance by combining the strengths
of multiple perception algorithms, support knowledge-enabled
reasoning about objects and enable automatic and knowledge-
driven generation of processing pipelines.
ROBOSHERLOCK uses annotations in form of logical
atoms in order to be able to ask structured queries and
perform logical reasoning about annotations added by the
AEs. The main reasoning engine used in ROBOSHERLOCK is
based on SWI-Prolog, a general purpose logic programming
language providing bindings for the most common proce-
nural languages like C++, Java. AEs can be equipped with
Prolog programs, which consist of facts and rules. Relevant
annotations are automatically asserted as facts into the Prolog
knowledge base.


@inproceedings{beetz2015robosherlock,
  title={RoboSherlock: Unstructured information processing for robot perception},
  author={Beetz, Michael and Balint-Benczedi, Ferenc and Blodow, Nico and Nyga, Daniel and Wiedemeyer, Thiemo and Marton, Zoltan-Csaba},
  booktitle={Robotics and Automation (ICRA), 2015 IEEE International Conference on},
  pages={1549--1556},
  year={2015},
  organization={IEEE}
}


Perspective Taking:
Abstract— In human-robot interaction, a robot must be
prepared to handle possible ambiguities generated by a human
partner. In this work we propose a set of strategies that allow a
robot to identify the referent when the human partner refers to
an object giving incomplete information, i.e. an ambiguous de-
scription. Moreover, we propose the use of an ontology to store
and reason on the robot’s knowledge to ease clarification, and
therefore, improve interaction. We validate our work through
both simulation and two real robotic platforms performing two
tasks: a daily-life situation and a game.

@inproceedings{Ros2010b,
  author = "Ros, R. and Lemaignan, S. and Sisbot, E.A. and Alami, R. and Steinwender, J. and Hamann, K. and Warneken, F.",
  booktitle = "{19th IEEE International Symposium in Robot and Human Interactive Communication}",
  title = "{Which One? Grounding the Referent Based on Efficient Human-Robot Interaction}",
  year = "2010"
}


Plan\intention Recognition:
@inproceedings{talamadupula2014coordination,
  title={Coordination in human-robot teams using mental modeling and plan recognition},
  author={Talamadupula, Kartik and Briggs, Gordon and Chakraborti, Tathagata and Scheutz, Matthias and Kambhampati, Subbarao},
  booktitle={Intelligent Robots and Systems (IROS 2014), 2014 IEEE/RSJ International Conference on},
  pages={2957--2962},
  year={2014},
  organization={IEEE}
}

@article{baker2014modeling,
  title={Modeling human plan recognition using Bayesian theory of mind},
  author={Baker, Chris L and Tenenbaum, Joshua B},
  journal={Plan, activity, and intent recognition: Theory and practice},
  pages={177--204},
  year={2014}
}